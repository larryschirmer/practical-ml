import os
import pickle
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

dir_path = os.path.dirname(os.path.realpath(__file__))


def makeLabledDatasets(file_names):
    """Convert files into list tuples
    ---
    Splits files by line and pairs the index of the file with the line

    (ipsum lorium, 0), (lorium ipsum, 1)

    file_names : one dimentional list
        list of the file names defines the order of the indexing
    """
    labeled_datasets = []

    for i, file_name in enumerate(file_names):
        lines_dataset = tf.data.TextLineDataset(
            os.path.join(dir_path, file_name))
        labeled_dataset = lines_dataset.map(
            lambda line: (line, tf.cast(i, tf.int64)))
        labeled_datasets.append(labeled_dataset)

    return labeled_datasets


def processData(labeled_datasets, buffer_size):
    """Convert list of map datasets into a single shuffled dataset
    ---
    Takes a list of mapped datasets containing string int64 tuples
    and recombines them into a single shuffled list

    labeled_datasets : list of mapped datasets
        datasets generated by makeLabeledDatasets
    buffer_size : int
        how many words to sample from while shuffling
    """
    labeled_data = labeled_datasets[0]

    for labeled_dataset in labeled_datasets[1:]:
        labeled_data = labeled_data.concatenate(labeled_dataset)

    labeled_data = labeled_data.shuffle(
        buffer_size, reshuffle_each_iteration=False)

    return labeled_data


def tokenizeDataset(labeled_data):
    """Tokenizes the dataset and returns the vocab set and how big it is
    ---
    Takes a uniform dataset of string, int tuples and splits the string
    into single words which it adds to the vocab

    labeled_data : shuffled list of string int tuples
        dataset generated by processData
    """
    tokenizer = tfds.features.text.Tokenizer()
    vocab_set = set()

    for text_tensor, _ in labeled_data:
        text = text_tensor.numpy()
        tokens = tokenizer.tokenize(text)
        vocab_set.update(tokens)

    vocab_size = len(vocab_set) + 1

    return vocab_size, vocab_set


def encodeDataset(labeled_data, vocab_set):
    """Assigns a int label to each vocab word in dataset
    ---
    Uses vocab set to assign an int label to each word

    labeled_data : shuffled list of string int tuples
        dataset generated by processData
    vocab_set : set of vocab words
        set generated by tokenizeDataset
    """
    encoder = tfds.features.text.TokenTextEncoder(vocab_set)

    def encode(text, label):
        return encoder.encode(text.numpy()), label

    def map_encoder(text, label):
        return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))

    return labeled_data.map(map_encoder)


def generateTrainingSets(encoded_data, opts):
    """Generates the training and test datasets for model training and evaluation
    ---
    Takes the encoded data and options and returns a shuffled and padded dataset

    encoded_data : tuple list of encoded words and file index
        ([list of encoded ints], int file index)
    opts : tuple containing training options
        (take_size, buffer_size, batch_size)
    """
    take_size, buffer_size, batch_size = opts
    train_data = encoded_data.skip(take_size).shuffle(buffer_size)
    train_data = train_data.padded_batch(batch_size, padded_shapes=([-1], []))

    test_data = encoded_data.take(take_size)
    test_data = test_data.padded_batch(batch_size, padded_shapes=([-1], []))

    return train_data, test_data


def plotHistory(history, filename='history.png'):
    plt.close('all')
    fig = plt.figure()

    loss = fig.add_subplot(121)
    loss.set_title('loss')
    loss.plot(history['loss'], '.', label="training loss")
    loss.plot(history['val_loss'], 'b', label="calidation loss")

    accuracy = fig.add_subplot(122)
    accuracy.set_title('accuracy')
    accuracy.plot(history['accuracy'], '.', label="training accuracy")
    accuracy.plot(history['val_accuracy'],
                  'b', label="validation accuracy")

    plt.tight_layout()
    fig.savefig(os.path.join(dir_path, filename))


def save(model, filename='saved_models/model.h5'):
    model.save(os.path.join(dir_path, filename))


def load(filename='saved_models/model.h5'):
    return tf.keras.models.load_model(os.path.join(dir_path, filename))


def saveVocab(vocab, filename='saved_models/vocab.pickle'):
    instance = open(os.path.join(dir_path, filename), "wb")
    pickle.dump(vocab, instance)
    instance.close()


def loadVocab(filename='saved_models/vocab.pickle'):
    instance = open(os.path.join(dir_path, filename), 'rb')
    vocab = pickle.load(instance)
    instance.close()

    return vocab
